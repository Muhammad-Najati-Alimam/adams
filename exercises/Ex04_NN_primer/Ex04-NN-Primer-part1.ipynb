{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/Ex04_NN_primer/Ex04-NN-Primer-part1.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #4: Neural Networking (NN) Primer (part 1)\n",
    "The elaborates on concepts covered in the Foundations of NN and NN Training lectures. It splits into two parts, the outline of which is as follows:\n",
    "\n",
    "## Phase 1 : setting up a NN ##\n",
    "\n",
    " 1. use case\n",
    " 2. from logit to neural network\n",
    " 3. nn structure: weights and biases\n",
    " 3. activation function\n",
    " 4. softmax\n",
    " 5. application\n",
    "\n",
    "\n",
    "## Phase 2 : make it work ##\n",
    " 1. loss function\n",
    " 2. gradient\n",
    " 3. weight update\n",
    " 4. learning rate\n",
    " 5. stochastic gradient descent and backpropagation\n",
    " 6. application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: setting up a NN # \n",
    "\n",
    "## 1.1 use case: app rating analysis ##\n",
    "NNs often solve classification problems with many classes. To illustrate their functioning in a context that is more related to later NLP examples, we will leave the space of credit scoring and introduce a new data set concerned with app ratings. The original data is available on [Kaggle](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps). There, you also find a data dictionary with some information on the variables. However, the variable names are rather self-explanatory. A cleaned version of the data is available in Moodle. The easiest way for you is to use the Moodle version but feel very free to play around with the version on Kaggle. You can find a Python script that starts from the original data and goes through a couple of pre-processing operations to produce the version we use below in our GitHub ([app_store_dpp.py](https://github.com/Humboldt-WI/adams/tree/master/exercises/app_store_dpp.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "app = pd.read_csv(\"../../data/AppleStore_prep.csv\",index_col=False,sep='\\t', encoding='utf-8')\n",
    "app.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Select the app name as row index\n",
    "app=app.set_index('track_name')\n",
    "app.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 36 columns, one of which is the target variable. We consider the user rating as our target and will try to set up a NN that predict the rating from the app characteristics including information on its prince, genre, etc. \n",
    "\n",
    "Let's have a quick look at the target variable. Note that we that we have reduced the number of rating classes compared to the original version of the data on Kaggle (see [app_store_dpp.py](https://github.com/Humboldt-WI/adams/tree/master/exercises/app_store_dpp.py) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the target\n",
    "import seaborn as sns\n",
    "sns.countplot(app.user_rating);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 From logit to NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a classification task, we are looking for a probability of a certain outcome. Here is the sigmoid function where *p* stands for target probability:\n",
    "$$p=\\frac{1}{1+e^{-y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression formula would then be:\n",
    "\n",
    "$$y=ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+...+\\beta_{k}X_{k}+e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matrix notation, this will look like:\n",
    "\n",
    "$$\\begin{bmatrix}  y_{1}\\\\  y_{2}\\\\   \\vdots\\\\ y_{n}\\\\ \\end{bmatrix}=\\begin{bmatrix} 1 & x_{11}&\\ldots\\ & x_{1,k}\\\\  1 & x_2\\\\  \\vdots & \\vdots\\\\  1 & x_n&\\ldots\\ & x_{n,k}\\\\ \\end{bmatrix}\\begin{bmatrix}  \\beta_{0}\\\\  \\beta_{1}\\\\   \\vdots\\\\ \\beta_{k}\\\\ \\end{bmatrix} +\\begin{bmatrix}  e_{1}\\\\  e_{2}\\\\   \\vdots\\\\ e_{n} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that logistic regression can be considered a one-layer neural network:\n",
    "\n",
    "<img src=\"LogtNN.png\" alt=\"logit\" style=\"width: 600px;\"/>\n",
    "\n",
    "Also note that $y$ in the above equation denotes the log-odds. Therefore, we actually depict a linear model, which we can equally state in matrix from as:\n",
    "\n",
    "$$y=WX+b$$\n",
    "\n",
    "where $W$ is the **weight** matrix, containing coefficients/betas for the covariates, and b is an error term or **bias**. To be precise, all of the above assumes $y$ to be the log-odds of a binary outcome variable. Things are a little more complicated when dealing with a multi-class problem. But let's not go there. Instead, depict our multi-class problem graphically as a NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnn.png\" alt=\"nn\" style=\"width: 1200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So have 35 variables and a target lable with 6 levels. How do we make it a NN task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 NN architecture ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with so called \"vanilla neural network\", or ANN.\n",
    "This is not deep learning yet, as we have just one \"hidden layer\".\n",
    " - a - input layer (number of neurons defined by number of features)\n",
    " - a' - hidden layer (number of neurons depends on the task, see best practice)\n",
    " - a'' - output layer (number of neurons depends on the type of output, 1 for regression, 2 for binary logistic regression, multiclass by number of classes)\n",
    "\n",
    "<img src=\"nn_full.png\" alt=\"fff\" style=\"width: 800px;\">\n",
    "\n",
    "\n",
    "Additionally, every neuron of a hidden layer has an **activation function** that transforms the input. \n",
    "For example, if the activation function is $f(x)=x^2$ then with input $x=2$, the neuron will take the value of 4.\n",
    "\n",
    "However, the usual \"go to\" functions for neural networks in recent years have been either *sigmoid*, *tanH*,  or *ReLu*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 activation function ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid function \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1-s)\n",
    "\n",
    "plt.plot(x, sigmoid(x), label='f(x)')\n",
    "plt.plot(x, sigmoid_derivative(x), label=\"f(x)'\")\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperbolic tangent\n",
    "def tanH(x):\n",
    "    return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "\n",
    "def tanH_derivative(x):\n",
    "    return 1-tanH(x)**2\n",
    "\n",
    "plt.plot(x, tanH(x), label=\"f(x)\")\n",
    "plt.plot(x, tanH_derivative(x), label=\"f'(x)\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#RELU\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (ReLU(x)>1).astype(int)\n",
    "\n",
    "    \n",
    "plt.plot(x, ReLU(x), label=\"f(x)\")\n",
    "plt.plot(x, ReLU_derivative(x), label=\"f'(x)\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have here a fully connected layer. The connections between layers represent **2 weight matrices**:\n",
    "\n",
    "<img src=\"nn_w.png\" alt=\"ffcf\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the matrix notation:\n",
    "\n",
    "<img src=\"m_notation.png\" alt=\"mn\" style=\"width: 800px;\">    \n",
    "\n",
    "And a single element computation:\n",
    "$$a'_{0}= f(w_{0,0}a_{0}+w_{0,1}a_{1}+w_{0,2}a_{2}+w_{0,3}a_{3} +...+ b_{0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix} \n",
    "a'_{0}\\\\ a'_{1}\\\\ \n",
    "\\vdots \\\\ \n",
    "a'_{h}\\\\\n",
    "\\end{pmatrix} =\n",
    "f \\Bigg(\n",
    "\\begin{pmatrix} \n",
    "w_{0,0} & w_{0,1} & \\ldots & w_{0,k} \\\\ \n",
    "w_{1,0} & w_{1,1} &\\ldots & w_{1,k}\\\\ \n",
    "w_{2,0} & w_{2,1} &\\ldots & w_{2,k}\\\\ \n",
    "\\vdots & \\vdots & \\ldots & \\vdots\\\\\n",
    "w_{h,0} & w_{h,1} & \\ldots & w_{h,k}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_{0}\\\\ a_{1}\\\\ \\vdots\\\\ a_{k}\\\\\n",
    "\\end{pmatrix}+\n",
    "\\begin{pmatrix}b_{0}\\\\  b_{1}\\\\\n",
    "\\vdots\\\\\n",
    "b_{h}\\\\\n",
    "\\end{pmatrix}\n",
    "\\Bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output will not quite give us the desired result. \n",
    "The above computations will provide us with **scores**, while we are in need of probabilities, \n",
    "and not just probabilities, but the distribution of probabilities over the classes of\n",
    " our target variable. That's where **softmax** comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 softmax function ##\n",
    "Recall the softmax function:\n",
    "$$S(x_{c})=\\frac{e^{x_{c}}}{\\sum_{j} e^{x_{j}}}$$\n",
    "\n",
    "We give it a score $x_c$ representing the confidence of the model that instances $x$ belongs to class $c$. We then normalize the score across all classes. In Python, this looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we obtained some scores from our NN. Let's compute the softmax output for these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "scores = [-100, -10, -0.5, -0.1, 0, 0.21]\n",
    "probs = softmax(scores)\n",
    "np.round(probs,4)  # round to make the output more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# and of course, these outputs sum to one\n",
    "sum(softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a nice picture of the computation, which also hints as the next step. Once we have our model predictions, which will be estimated class probabilities thanks to softmax, we will compare these to the actual class labels and compute a loss. You will see the latter step later in the tutorial:\n",
    "<img src=\"softmax.png\" alt=\"ffff\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [RitchiNg.com](https://www.ritchieng.com/machine-learning/deep-learning/neural-nets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.6 Application\n",
    "Let's try to set up the NN architecture on our own! We have already loaded our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "len(app) # that would be the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# And here is one observation\n",
    "app.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our data for the NN, we need to encode the labels. Currently, these are just numbers running from 0 to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "app[\"user_rating\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our NN, we need the class labels as a matrix of dummies. In other words, we need to apply one-hot-coding to our labels. This facilitates calculating the loss of the NN using cross-entropy. The one-hot-coding is easily done using a keras utility function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "from keras.utils import np_utils\n",
    "dummy_y = np_utils.to_categorical(app[\"user_rating\"])\n",
    "dummy_y  # 6 levels -> 6 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that is enough we could make the case a little more comprehensive. Say your labels are not integers but classes, as in rating analysis for example where we have rating grades like AAA, AA, A, ..., D. Here is a standard workflow for dealing with labels in  multi-class problems. We first encode them to integers and then use one-hot-coding. Although it is an overkill for the focal case, it is good to see the typical process for once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# first encode the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(app[\"user_rating\"])\n",
    "encoded_Y = encoder.transform(app[\"user_rating\"])\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "from keras.utils import np_utils\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y  # 6 levels -> 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Some more housekeeping before we start with the NN\n",
    "data = np.array(app)  # convert to numpy array\n",
    "X = data[:,:-1]  # matrix of covariates only \n",
    "y = dummy_y.astype(int)  # type conversion of the target\n",
    "\n",
    "# Done, here is the final target for the sixth app in the data frame. Remeber which one that was?\n",
    "y[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's set up the structure of our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the structure of the NN\n",
    "inputLayer = 35  # by the number of features - a\n",
    "hiddenLayer = 50 # hidden layer, as we decided - a'\n",
    "outputLayer = 6  # output, by the number of levels - a''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is our NN structure again.\n",
    "\n",
    "<img src=\"nn_ws.png\" alt=\"ffcf\" style=\"width: 800px;\">\n",
    "\n",
    "Apart from the input, hidden, and output dimension, we need to initialize the weight matrices. Completely random initialization will not work well. To do a little better than that, we use uniformly distributed random numbers in a bounded range. In a nutshell, this is to avoid neurons getting saturated. Glorot and Bengio (2010) and some deep learning tutorials set the boundary for random numbers based on the  number of incoming and outgoing connections:\n",
    "\n",
    "$$ r = \\sqrt{\\frac{6}{\\text{in} + out}} $$\n",
    "\n",
    "where $in$ and $out$ represent the number of incoming and outgoing connections, respectively. You can then initialize weights to random numbers in the range $(-r,r)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Facilitate replication by fixing the random number seed\n",
    "np.random.seed(123)\n",
    "\n",
    "limit = np.sqrt(6 / (inputLayer + outputLayer)) \n",
    "\n",
    "weightsInputToHidden  = np.random.uniform(-limit, limit, (hiddenLayer, inputLayer)) # W in the picture\n",
    "weightsHiddenToOutput = np.random.uniform(-limit, limit, (outputLayer, hiddenLayer)) # M in the picture\n",
    "\n",
    "biasInputToHidden  = np.ones( (hiddenLayer,1) ) # for sigmoid to pick 0, for ReLu you pick one - b\n",
    "biasHiddenToOutput = np.ones( (outputLayer,1) ) #  v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be inputing just one observation for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#X[5]\n",
    "app.iloc[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We need to ensure that the input comes as an array of correct dimension\n",
    "print(X[5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This will not work since, for matrix multiplication, we need a kx1 vector\n",
    "inputs = np.array(X[5]).reshape( (35,1) )\n",
    "inputs.shape # now we have a nice and sweet input vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we need to make sure our output vectors will be what we want it to be. \n",
    "# That is basically the same operation as above\n",
    "print(y[5].shape)\n",
    "target = y[5].reshape( (6,1) )\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show matrices of weights\n",
    "weightsInputToHidden.shape, inputs.shape, weightsHiddenToOutput.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we are all set, let's multiply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "hL_outputs = ReLU(hL_inputs)\n",
    "hL_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput\n",
    "oL_inputs\n",
    "#doesn't look like probabilities, that's because these are our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "oL_outputs = softmax(oL_inputs)\n",
    "oL_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and just for verification\n",
    "np.sum(oL_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## So what was the label again?\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model is somewhat on the way but still a little uncertain, we need to give it a nudge in the right direction. So how do you usually tell the model it's wrong? You show it the evaluation metric and try to make it better.\n",
    "\n",
    "To evaluate the the probabilities estimated by the model, we need to compare them with the true values, \n",
    "that are one-hot encoded in our case. So we need to compare our outputs to \n",
    "\n",
    "[ 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "You can imagine that we cannot use MSE here, as we are dealing with a certain probability distribution.\n",
    "Our *loss function* has to grasp the difference between these probabilities. This is where the **cross-entropy loss**\n",
    "function will come in handy. It is used when the output of a model represents the probability of an outcome, i.e. when the output is a probability distribution. \n",
    "It is used as a loss function in neural networks that have softmax activations in the output layer.\n",
    "\n",
    "*Remark*: in case you are interested, there a nice article (with codes) on the differences between squared-error and cross-entropy\n",
    "in the context of training neural networks appeared in the [Visual Study magazine](https://visualstudiomagazine.com/articles/2017/07/01/cross-entropy.aspx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: make it work # \n",
    " 1. loss function\n",
    " 2. gradient\n",
    " 4. batches\n",
    " 5. learning rate\n",
    " 6. stochastic gradient descent\n",
    " 7. backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loss function\n",
    "### Logistic regression case\n",
    "Let us revisit classic logistic regression. We build a linear model to distinguish between two classes. So our target variable can only take one of two states $ y \\in \\{0,1\\} $.\n",
    "\n",
    "With $ w$ denoting the parameters (aka weights) of our logit model and letting observations be indexed by $i = 1, 2, ..., n$, our loss function $J(w)$ has the form:\n",
    "\n",
    "$$J(w) = \\sum_{i=1}^{n} y^{(i)} \\log P(y=1) + (1 - y^{(i)}) \\log P(y=0)$$\n",
    "Where P(y) represent the probability of a certain binary outcome\n",
    "\n",
    "We will however consider another loss function - cross entropy. It is used when the output represents the probability of an outcome, i.e. when the output is a probability distribution. It is used as a loss function in neural networks that have softmax activations in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy \n",
    "Entropy ($H(y)$) is a term from Information Theory. It had a great impact on the field of communication and signifies the optimal number of bits to encode a certain information content ($y_c$ is the probability of the c-th event, symbol or in our case class):\n",
    "\n",
    "$$H(y) = \\sum_c y_c \\log \\frac{1}{y_c} = -\\sum_c y_c \\log y_c$$\n",
    "\n",
    "Now the cross-entropy ($H(y,\\hat{y})$) is the number of bits we'll need if we encode symbols from $y$ using the wrong tool $\\hat{y}$. Cross entropy is always bigger or equal to entropy. Mind that $c$ stands for the number of classes. \n",
    "\n",
    "$$H(y, \\hat{y}) = \\sum_c y_c \\log \\frac{1}{\\hat{y}_c} = -\\sum_c y_c \\log \\hat{y}_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, the The KL divergence that you have encountered before in BADS (uplift random forest) is simply the difference between cross entropy and entropy:\n",
    "$$\\mbox{KL}(y~||~\\hat{y})\n",
    "= \\sum_c y_c \\log \\frac{1}{\\hat{y}_c} - \\sum_c y_c \\log \\frac{1}{y_c}\n",
    "= \\sum_c y_c \\log \\frac{y_c}{\\hat{y}_c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would be calculating the cross-entropy for every pair of true/estimated probabilities and averaging it over the sample or batch (more about it later) - this will be our loss function *L* that we will ultimately want to minimise (class i, smaple j):\n",
    "\n",
    "$$L=-\\frac{1}{N}\\sum_i \\sum_c y_{i,c} \\log(\\hat{y}_{i,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CE example\n",
    "A concrete example is the best way to explain the purely mathematical form of CE. Suppose you have a weirdly shaped\n",
    "four-sided dice (yes, I know the singular is really \"die\"). Using some sort of intuition or physics, you predict\n",
    "that the probabilities of the four sides are (0.20, 0.40, 0.30, 0.10). \n",
    "You then roll the dice many thousands of times and determine that the true probabilities are  (0.15, 0.35, 0.25, 0.25). \n",
    "Here is how we calculate the CE error of our prediction:\n",
    "\n",
    "CE prediction error:\n",
    "-1.0 * [ ln(0.20) * 0.15 + ln(0.40) * 0.35 + ln(0.30) * 0.25 + ln(0.10) * 0.25 ] = \n",
    "-1.0 * [ (-1.61)(0.15) + (-0.92)(0.35) + (-1.20)(0.25) + (-2.30)(0.25) ] =\n",
    "1.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ce=-1.0 * (np.log(0.20) * 0.15 + np.log(0.40) * 0.35 + np.log(0.30) * 0.25 + np.log(0.10) * 0.25)\n",
    "print('Cross entropy error of weird dice prediction is: {:.4f}'.format(ce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply this idea to our network outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yhat=np.round(oL_outputs, 3) #that's our WX+b run through softmax\n",
    "print('True label: ',  target.shape)\n",
    "print(target)\n",
    "print('Prediction: ', yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculation of cross-entropy loss\n",
    "L =- sum(target*np.log(yhat))\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or, if you prefer\n",
    "L=-1*np.dot(target.T, np.log(yhat))\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number we want the machine to minimize! But how?\n",
    "We ca not change our input, we cannot change our labels, what we can adjust are **weights and biases** - these are the parameters of our neural network. So we need to change our **W, M, b and v** and check if the loss decreases.\n",
    "\n",
    "NB: number of neurons in the hidden layer AND number of hidden layers are meta-parameters and certainly can be experimented with, we will discuss it further down the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to move around the function slope is to find the derivative.\n",
    "\n",
    "In our naive example, we are dealing with 1 example only while we have 6 classes. In practice, these numbers, the number of examples and output classes would be much larger, making the calculation of the derivative of the loss function computationally demanding. \n",
    "\n",
    "And that was the point when everybody almost gave up on neural networks. Spoiler alert: the solutions were **stochastic gradient descent and backpropagation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient \n",
    "The gradient ( $\\nabla$) is a vector operation which operates on a scalar function to produce a vector whose magnitude is the maximum rate of change of the function at the point of the gradient and which is pointed in the direction of that maximum rate of change. \n",
    "\n",
    "Well, put in easier terms, gradient is a **vector of partial derivatives**. Why would we need it? Because we need the derivative of this:\n",
    "$$L=-\\frac{1}{N}\\sum\\sum y\\log(softmax(M(relu(Wx+b))+v)),$$\n",
    "\n",
    "where the functions $s$ and $r$ stand for softmax, and relu, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We want to adjust every estimation of y (yhat) so as to minimize the loss function. For that we would deduct a gradient of the loss function from it and continue doing these iterations until we reach the local minimum. \n",
    "$$L_{t+1} =L_t - \\eta \\cdot \\nabla L(W, M, b, v)$$\n",
    "\n",
    "You can think of of gradient as a list of directions for improvement (of course, imagining moving in 10000 directions is hard):\n",
    "$$ \\nabla L(W,M,b,v)=\\begin{bmatrix}  \\frac{dL}{dW}\\\\ \\frac{dL}{dM}\\\\ \\frac{dL}{db}\\\\ \\frac{dL}{dv}\\end{bmatrix}$$\n",
    "\n",
    "Normally it would be a result of averaging over all observations that went through the forward pass, but again- we have just one this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent example\n",
    "For illustration purpose, we will try to find the local minimum of the function $ f(x) = \\left(x-4\\right)^2 $. In this case, the gradient is a simple derivative: $2 \\left(x-4\\right)$.\n",
    "\n",
    "The example draws on: https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cur_x = 10 # Randomly guessed starting point \n",
    "rate = 0.1 # Learning rate\n",
    "\n",
    "# Let's impose two stopping conditions, a max. number\n",
    "# of iterations and a minimum step size\n",
    "precision = 0.000001 # threshold for the step size\n",
    "previous_step_size = 1 # change of x in the prev. step\n",
    "max_iters = 10 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "\n",
    "df = lambda x: 2*(x-4) #Gradient of our function '\n",
    "\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    prev_x = cur_x #store current x value in prev_x\n",
    "    cur_x = cur_x - rate * df(prev_x) # gradient descent step\n",
    "    previous_step_size = abs(cur_x - prev_x) # change in x\n",
    "    iters = iters+1 #iteration count\n",
    "    if (iters%5 == 0.0):\n",
    "        print(\"Iteration {:02d} with X equal to {:.4f}\".format(iters,cur_x)) #Print iterations\n",
    "    \n",
    "print(\"The local minimum occurs at: {:.10f}\".format(cur_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3 updating weights\n",
    "The following codes sketch gradient descent for neural network learning. For simplicity, we consider the squared-error loss function. Its derivative is very easy and more handy compared to cross-entropy. \n",
    "\n",
    "Furthermore, since we minimize squared-error, we do not need the softmax layer anymore. Instead, we could   minimize the error for the raw output scores of the NN. The softmax derivative is a little complicated. Thus, avoiding softmax simplifies our example. Rather, we don't avoid it entirely but replace the softmax function with ReLU. ReLU ensures that our network outputs are strictly positive. We could still interpret these outputs as unnormalized confidences for the different classes. That is perfectly fine for this example. After all, the concepts illustrated below also apply to the cross-entropy loss function or yet other loss functions and networks with softmax layer.\n",
    "\n",
    "Let's try to write down our simplified modeling problem:\n",
    "\n",
    "$$L=-\\frac{1}{N}\\sum\\sum \\frac{1}{2}(y - relu(M(relu(Wx+b))+v)^2 ),$$\n",
    "\n",
    "To minimize this loss function, we need its derivative, which is equal to the model residuals, and the derivative of our ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def loss_derivative(output, y): #check the derivation if interested https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/\n",
    "    return output - y\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0)*1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third lecture, we went through the steps in backpropagation. You might want to revisit the steps and have the slides ready when going through the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we reproduce the steps in the calculation of the forward path from above\n",
    "inputs = np.array(X[5]).reshape( (35,1) )  # one example from the data set\n",
    "target = y[5].reshape( (6,1) )  # one-hot-coded app rating\n",
    "hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden  # input to hidden layer\n",
    "hL_outputs = ReLU(hL_inputs)  # hidden layer output\n",
    "oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput  # input to output layer\n",
    "oL_outputs = ReLU(oL_inputs)  # above we used softmax, as said, we simplify the example and use ReLU\n",
    "oL_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we do the backward pass and work out gradients. We begin with the weight matrix M, that connects the hidden to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We first need the residuals, which are equal to the derivative of our loss\n",
    "dL_dyhat = loss_derivative(oL_outputs, target)\n",
    "dL_dyhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Next we need the derivative of the predictions wrt the output layer activation; normally softmax and here ReLU for simplicity\n",
    "dyhat_dact = ReLU_derivative(oL_inputs)\n",
    "dyhat_dact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. The last factor in the chain is the derivative of the output layer input wrt the hidden layer output\n",
    "# This is simply the output of the hidden layer, so no computations needed.\n",
    "hL_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before moving on, let's revisit the dimensionality of our factors\n",
    "dL_dyhat.shape, dyhat_dact.shape, hL_outputs.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are set up to calculate the first gradient\n",
    "gradient_HiddenToOutput = np.dot(dL_dyhat * dyhat_dact, np.transpose(hL_outputs))\n",
    "gradient_HiddenToOutput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just what we need for our gradient update\n",
    "weightsHiddenToOutput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hidden layer also as biases and we need to update these, too. Following the same logit as above,\n",
    "# we arrive at,\n",
    "gradient_HiddenToOutput_bias = dL_dyhat * dyhat_dact\n",
    "# To see this, go back to the above equation and note that our bias, v, enters the loss function. We do not\n",
    "# need to consider the output of the hidden layer in our gradient computation. Everything else remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already found two parts of our gradient. Let's proceed with the weight matrix W, connecting the hidden and the input layer. We need to go through similar steps as before. More specifically, we begin exactly as before and find the derivative of the loss wrt the NN prediction and the derivative of the prediction wrt our ReLU output layer activation. Remember that caching intermediate results is crucial in the scope of  backpropagation.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already performed this computation twice and now we need it again. Let's finally store the result\n",
    "# Yes, we could and should have done this earlier; but doing things wrong for once helps to learn ;)\n",
    "# We can give our cached results any name we like but the term output layer error is commonly used\n",
    "oL_errors = dL_dyhat * dyhat_dact  \n",
    "print('Shape output layer error {}'.format(oL_errors.shape))\n",
    "# When backpropagating errors further down our network, the next factor we need is the derivative of the \n",
    "# output layer input wrt the hidden layer output. This is the weight matrix M, because of the linear \n",
    "# relationship between these two quantities. \n",
    "print('Shape of M {}'.format(weightsHiddenToOutput.shape))\n",
    "gradient_InputToHidden = np.dot(weightsHiddenToOutput.T, oL_errors)\n",
    "\n",
    "# Thereafter, we add another factor, which is the derivative of the hidden layer output wrt the\n",
    "# hidden layer input. So we need our ReLU derivative again\n",
    "gradient_InputToHidden = gradient_InputToHidden * ReLU_derivative(hL_inputs)\n",
    "print('Shape of gradient vector {}'.format(gradient_InputToHidden.shape))\n",
    "\n",
    "# Finally, the hidden layer inputs depend on the weights in a linear manner. The derivate of the hidden\n",
    "# layer input wrt to the weight matrix W, which gradient we seek, is equal to the inputs x\n",
    "print('Shape of input {}'.format(inputs.shape))\n",
    "gradient_InputToHidden = np.dot(gradient_InputToHidden, np.transpose(inputs)   )\n",
    "gradient_InputToHidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the gradient of the loss wrt the hidden layer bias is again a little easier.\n",
    "# We go through the same steps as above but the last step. The inputs to not enter the calculation\n",
    "gradient_InputToHidden_bias = np.dot(weightsHiddenToOutput.T, oL_errors) * ReLU_derivative(hL_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that every step in the calculation of $\\frac{dL}{db}$ repeats a large part of the calculation of $\\frac{dL}{dW}$ informs us that we have once again missed an opportunity to write efficient code.\n",
    "\n",
    "Finally, we have collected all gradients and can update the weights using the standard gradient descent rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "learningRate = 0.0001  # define some learning rate\n",
    "\n",
    "# Update M\n",
    "weightsHiddenToOutput -= learningRate * gradient_HiddenToOutput\n",
    "# Update v\n",
    "biasHiddenToOutput -= learningRate * gradient_HiddenToOutput_bias \n",
    "# Update W\n",
    "weightsInputToHidden -= learningRate * gradient_InputToHidden\n",
    "# Update b\n",
    "biasInputToHidden -= learningRate * gradient_InputToHidden_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Stochastic gradient descent and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from derivatives, avereging over the whole training dataset might be costly. That is why **stochastic** gradient descent was introduced. It basically means that instead of using the full training set, the algorithm will only use a certain  random **batch** (size of this batch is a metaparameter like the number of neurons). This introduces a certain \"slopinness\" to the process but allows to run the **backpropagation** much faster. After many iterations, we expect to converge to the sample parameters anyway. Below, we exemplify backpropagation for our simple neural network. To that end, we require another important concept, an **epoch**. An epoch describes on run of the entire data set through the NN. \n",
    "\n",
    "Essentially, Backpropagation is an algorithm for computing the gradient in a multidimensional space. You can find a very good description of the algorithm in the [ML Glossary](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html). Here is a picture illustrating the idea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn_color.png\" alt=\"Backpropagation\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5 Updating for each observation in the data (the original stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learningRate = 0.0001\n",
    "epochs = 10 # how many times do we have to run the training set through the network\n",
    "\n",
    "input_dim = X.shape[1] # number of variables\n",
    "n_classes = y.shape[1]  # number of classes (i.e., 6 user ratings one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iteration=0\n",
    "\n",
    "while iteration < epochs:\n",
    "    \n",
    "    # Put all the steps done before in a loop:\n",
    "    # Process one observation per iteration \n",
    "    squared_residuals = np.zeros(X.shape[0]) # to keep track of the loss\n",
    "    for i in range(X.shape[0]):\n",
    "\n",
    "        inputs  = np.array(X[i]).reshape( (input_dim,1) ) # select current case i\n",
    "        target = y[i].reshape( (n_classes,1) ) # and its target value\n",
    "                \n",
    "        # Compute the forward pass through the network all the way up to the final output  \n",
    "        hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "        hL_outputs = ReLU(hL_inputs)\n",
    "        oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput \n",
    "        oL_outputs = ReLU(oL_inputs) # final output: probabilistic prediction for case i \n",
    "\n",
    "        # Compute the gradients:\n",
    "        # gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = loss_derivative(oL_outputs, target) * ReLU_derivative(oL_outputs)\n",
    "        oL_errors = gradient_HiddenToOutput\n",
    "        \n",
    "        # gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(weightsHiddenToOutput.T, oL_errors) * ReLU_derivative(hL_inputs)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        biasHiddenToOutput -= learningRate * gradient_HiddenToOutput\n",
    "        biasInputToHidden  -= learningRate * gradient_InputToHidden\n",
    "\n",
    "        weightsHiddenToOutput -= learningRate * np.dot(gradient_HiddenToOutput, np.transpose(hL_inputs))\n",
    "        weightsInputToHidden  -= learningRate * np.dot(gradient_InputToHidden, np.transpose(inputs))\n",
    "        \n",
    "        # Store residual \n",
    "        squared_residuals[i] = np.sum((target-oL_outputs)**2)\n",
    "        \n",
    "    # Development of the loss as average over obs-level losses\n",
    "    print('Epoch {} with loss {:.4f}'.format(iteration, np.mean(squared_residuals)))\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the end of our simplified manual backprob demo. After going through the code you should extend your skills by browsing through some more elaborate tutorials such as https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795 or https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's check the results! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract some 'test' data\n",
    "X_test = data[78:178,:-1]\n",
    "y_test = dummy_y[78:178].astype(int)\n",
    "y_test = np.argmax(dummy_y[78:178], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# scorecard of the network\n",
    "confusion_matrix = np.zeros( (6,6), dtype=\"int\" )\n",
    "true_labels = np.argmax(y, axis=1)\n",
    "pred_labels = np.zeros_like(true_labels)\n",
    "# go through all the observations in the test data set\n",
    "for i in range(len(y)):\n",
    "    # compute the network output (using the trained weights)\n",
    "    inputs = np.array(X[i]).reshape( (35,1) )\n",
    "\n",
    "    hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "    hL_outputs = ReLU(hL_inputs)\n",
    "    oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput\n",
    "    oL_outputs = softmax(oL_inputs)  # that is a bit of cheating since we trained using ReLU\n",
    "\n",
    "    # determine most likely class\n",
    "    pred_labels[i] = np.argmax(oL_outputs)\n",
    "\n",
    "    confusion_matrix[pred_labels[i], true_labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate percentage correctly classified\n",
    "pcc = np.trace(confusion_matrix) / len(y)\n",
    "print('Classification accuracy of trained network: {:.2f}'.format(pcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too bad for our first neural network and a lot better than random guessing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "guessed_class = np.random.randint(0,6, y.shape[0])\n",
    "m=pd.crosstab(guessed_class, true_labels)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('Classification accuracy of randomly guessing classes: {:.2f}.'.format(np.trace(m.to_numpy()) / len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Learning rate\n",
    "Learning rate is a **hyper-parameter** that controls how much we are adjusting the weights of our network with respect the loss gradient. \n",
    "Lower LR takes more time but allows better allocation of local minimum, higher LR allows faster calculations but drastic jumps do not always yield good results. \n",
    "        $$ newWeights=OldWeights - learningRate *gradientOfOldWeights$$ \n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/learningrates.jpeg\" alt=\"Drcng\" style=\"width: 400px;\"/>Img Credit: cs231n\n",
    "\n",
    "One can improve the results of computations significantly if learning rate is set well. However, learning rate might not remain the same throughout the training. The concpet of cyclical learning rate was introduced by Leslie N.Smith in 2015, it conveys a certain schedule when the LR starts with small values and increases (either linearly or exponentially) at each iteration. Learning rate decay would provide an alternative, it would bare the same problem though - when to decay the LR (step decay, exponential decay, others). In practice, step decay is preferred by many practitioners as hyperparameters it involves (the fraction of decay and the step timings in units of epochs) are more interpretable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Momentum\n",
    "\n",
    "$$\\Delta  W_{i} = -learningRate  \\frac{\\partial L}{\\partial W} + \\mu  \\Delta W_{i-1}$$\n",
    "\n",
    "\n",
    "The second part that contains $\\mu$ is a momentum term here (or coefficient), that defines the effect of the accumulated past gradient (we are taking an exponentially weighted moving average of accumulated updates). You can think of it as a certain velocity control mechanism. When we reach flatter areas, it will increase the speed of convergence, while dampening oscillations when reaching high curvatures. If the learning rate measures how much the current situation affects the next step, momentum measures how much past steps affect the next step. \n",
    "\n",
    "Conventional values to set for momentum is 0.5 increasing to 0.9, in case of cross validation can be set to values such as [0.5, 0.9, 0.95, 0.99]\n",
    "\n",
    "**Nesterov Momentum** is a slightly different version of the momentum update that has recently been gaining popularity. It is set as a meta-parameter in basic Keras application that we will see in the next tutorial. In simplified terms, Nesterov momentum gives gradient a better 'nudge' as it contains a 'lookahead' information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further study\n",
    "\n",
    "I strongly recommend watching this video from 3blue1brown on neural network training using gradient descent and backpropagation: https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLLMP7TazTxHrgVk7w1EKpLBIDoC50QrPS&index=4\n",
    "\n",
    "In my opinion, this is the single most intuitive explanation available on the internet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
